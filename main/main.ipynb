{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1 for dataset housing.\n",
      "\n",
      "\n",
      "Starting fold 2 for dataset housing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: ensure response column is \"-1\"\n",
    "\n",
    "# This file contains code to train dropout networks on the UCI datasets using the following algorithm:\n",
    "# 1. Create 20 random splits of the training-test dataset.\n",
    "# 2. For each split:\n",
    "# 3.   Create a validation (val) set taking 20% of the training set.\n",
    "# 4.   Get best hyperparameters: dropout_rate and tau by training on (train-val) set and testing on val set.\n",
    "# 5.   Train a network on the entire training set with the best pair of hyperparameters.\n",
    "# 6.   Get the performance (MC RMSE and log-likelihood) on the test set.\n",
    "# 7. Report the averaged performance (Monte Carlo RMSE and log-likelihood) on all 20 splits.\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from web_dataloader import web_dataloader\n",
    "from test_mc_dropout import test_mc_dropout\n",
    "from test_ngboost import test_ngboost\n",
    "\n",
    "seed= 42\n",
    "\n",
    "dataset_name = \"housing\"\n",
    "model = \"vogn\"\n",
    "n_folds = 2\n",
    "alpha = .05\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load Dataset\n",
    "dataset = web_dataloader[dataset_name]() #[args.dataset]()\n",
    "X, y = dataset.iloc[:, :-1].values, dataset.iloc[:, -1].values\n",
    "\n",
    "# Get row indices for the K different dataset splits\n",
    "folds = KFold(n_splits=n_folds, shuffle=False, random_state=seed).split(X)\n",
    "\n",
    "# # Log results in .txt file\n",
    "# with open(\"results_\"+model+\"/\"+dataset_name+\".csv\", \"w\") as myfile:\n",
    "#     myfile.write(\"coverage,avg_range,test_ll\\n\")\n",
    "\n",
    "# For each dataset split get train, validation, testing\n",
    "for fold_count, (train_index, test_index) in enumerate(folds):\n",
    "    print(f\"\\nStarting fold {fold_count+1} for dataset {dataset_name}.\\n\")\n",
    "\n",
    "    X_trainall, X_test = X[train_index], X[test_index]\n",
    "    y_trainall, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainall, y_trainall, test_size=0.2\n",
    "    )       \n",
    "\n",
    "    # Call testing function\n",
    "#     coverage, avg_range, test_ll=\\\n",
    "#         test_ngboost(X_train, y_train, X_val, y_val, X_test, y_test, alpha)\n",
    "\n",
    "#     # Write result\n",
    "#     with open(\"results_\"+model+\"/\"+dataset_name+\".csv\", \"a\") as myfile:\n",
    "#         myfile.write(repr(coverage)+\",\"+repr(avg_range)+\",\"+repr(test_ll)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchsso\n",
    "from perceptrons import MLP\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check GPU availability\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train= torch.from_numpy(X_train).type(torch.float)\n",
    "y_train= torch.from_numpy(y_train).type(torch.float)\n",
    "\n",
    "# Prepare data\n",
    "train_dataset= TensorDataset(\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "X_val = torch.from_numpy(X_val).type(torch.float)\n",
    "y_val = torch.from_numpy(y_val).type(torch.float)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).type(torch.float)\n",
    "y_test = torch.from_numpy(y_test).type(torch.float)\n",
    "\n",
    "train_loader= DataLoader(train_dataset, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_params = {'hidden_sizes': [50,50,50],\n",
    "                'act_func': \"relu\",\n",
    "                'prior_prec': 1.0}\n",
    "\n",
    "# Training parameters\n",
    "train_params = {'num_epochs': 40,\n",
    "                'batch_size': None,\n",
    "                'train_mc_samples': None,\n",
    "                'eval_mc_samples': 100,\n",
    "                'seed': 123}\n",
    "\n",
    "# Optimizer parameters\n",
    "optim_params = {'learning_rate': 0.001,\n",
    "                'betas': (0.9, 0.999),\n",
    "                'prec_init': 1.0}\n",
    "\n",
    "\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-607dc304895c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvogn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/AmazonSageMaker-SAM/uncertainty_konstantinos/external_data/vadam-master/pytorch/vadam/optimizers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# Get the diagonal of the GGN matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mggn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mgrad_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mggn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mggn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "from vadam.optimizers import VOGN\n",
    "import vadam.metrics as metrics\n",
    "\n",
    "# Normalize X        \n",
    "X_means = torch.mean(X_train, dim=0)\n",
    "X_stds = torch.std(X_train, dim=0)\n",
    "X_stds[X_stds == 0] = 1\n",
    "            \n",
    "# Normalize y\n",
    "y_mean = torch.mean(y_train)\n",
    "y_std = torch.std(y_train)\n",
    "if y_std==0:\n",
    "    y_std = 1\n",
    "\n",
    "# Initialize model\n",
    "vogn_model= MLP(input_size=X_train.shape[1], \n",
    "                output_size=1, \n",
    "                hidden_sizes=[50,50,50], \n",
    "                act_func='relu')\n",
    "\n",
    "# Initialize optimizer\n",
    "vogn_optimizer = VOGN(vogn_model.parameters(),\n",
    "                      lr = 1e-3,\n",
    "                      beta = .99,\n",
    "                      prior_prec = 1.,\n",
    "                      prec_init = 1.,\n",
    "                      num_samples = 10,\n",
    "                      train_set_size = X_train.shape[0])\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Set model in training mode\n",
    "    vogn_model.train(True)\n",
    "\n",
    "    # Initialize batch objective accumulator\n",
    "    batch_objective = []\n",
    "\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "        # Normalize x and y\n",
    "        X= (X- X_means)/ X_stds\n",
    "        y= (y-y_mean)/ y_std\n",
    "\n",
    "        # Update parameters\n",
    "        def closure():\n",
    "            vogn_optimizer.zero_grad()\n",
    "            output = vogn_model(X)\n",
    "            loss = metrics.avneg_loglik_gaussian(output, y, tau = 1.)\n",
    "            loss.backward()\n",
    "            return loss, output\n",
    "        loss, _ = vogn_optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.9000)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3068e+09, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vogn_model(X_train[1,])\n",
    "metrics.avneg_loglik_gaussian(output.squeeze(), y, tau = 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Store batch objective\n",
    "        batch_objective.append(loss.detach().cpu().item())\n",
    "        print(f'Train Epoch: {epoch+1}\\tLoss(VOGN): {loss:.6f}')\n",
    "\n",
    "#     # Compute and store average objective from last epoch\n",
    "#     self.objective_history[split].append(np.mean(batch_objective))\n",
    "\n",
    "#     if log_metric_history:\n",
    "\n",
    "#         # Set model in test mode\n",
    "#         self.model.train(False)\n",
    "\n",
    "#         # Evaluate model\n",
    "#         with torch.no_grad():\n",
    "#             self._evaluate_model(self.metric_history[split], x_train, y_train, x_val, y_val)\n",
    "\n",
    "#         # Print progress\n",
    "#         self._print_progress(split, epoch)\n",
    "\n",
    "#     else:\n",
    "\n",
    "#         # Print average objective from last epoch\n",
    "#         self._print_objective(split, epoch)\n",
    "\n",
    "# # Set model in test mode\n",
    "# self.model.train(False)\n",
    "\n",
    "# # Evaluate model\n",
    "# with torch.no_grad():\n",
    "#     self._evaluate_model(self.final_metric[split], x_train, y_train, x_val, y_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get train predictions\n",
    "# mu_list = vogn_optimizer.get_mc_predictions(vogn_model.forward, \n",
    "#                                             inputs = X_val, \n",
    "#                                             mc_samples = train_params['eval_mc_samples'], \n",
    "#                                             ret_numpy=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_var_quant_kostas",
   "language": "python",
   "name": "conda_var_quant_kostas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
